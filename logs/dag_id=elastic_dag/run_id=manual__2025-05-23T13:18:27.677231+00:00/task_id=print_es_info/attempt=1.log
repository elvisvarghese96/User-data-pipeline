[2025-05-23T13:18:28.888+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: elastic_dag.print_es_info manual__2025-05-23T13:18:27.677231+00:00 [queued]>
[2025-05-23T13:18:28.892+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: elastic_dag.print_es_info manual__2025-05-23T13:18:27.677231+00:00 [queued]>
[2025-05-23T13:18:28.892+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2025-05-23T13:18:28.892+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 1
[2025-05-23T13:18:28.892+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2025-05-23T13:18:28.897+0000] {taskinstance.py:1300} INFO - Executing <Task(PythonOperator): print_es_info> on 2025-05-23 13:18:27.677231+00:00
[2025-05-23T13:18:28.900+0000] {standard_task_runner.py:55} INFO - Started process 3691 to run task
[2025-05-23T13:18:28.902+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'elastic_dag', 'print_es_info', 'manual__2025-05-23T13:18:27.677231+00:00', '--job-id', '490', '--raw', '--subdir', 'DAGS_FOLDER/elastic_dag.py', '--cfg-path', '/tmp/tmpczcdvsqv']
[2025-05-23T13:18:28.903+0000] {standard_task_runner.py:83} INFO - Job 490: Subtask print_es_info
[2025-05-23T13:18:28.927+0000] {task_command.py:388} INFO - Running <TaskInstance: elastic_dag.print_es_info manual__2025-05-23T13:18:27.677231+00:00 [running]> on host 74c05f868d26
[2025-05-23T13:18:28.954+0000] {taskinstance.py:1509} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=elastic_dag
AIRFLOW_CTX_TASK_ID=print_es_info
AIRFLOW_CTX_EXECUTION_DATE=2025-05-23T13:18:27.677231+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-05-23T13:18:27.677231+00:00
[2025-05-23T13:18:28.957+0000] {base.py:73} INFO - Using connection ID 'elastic_default' for task execution.
[2025-05-23T13:18:28.957+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/elastic_dag.py", line 7, in _print_es_info
    hook = ElasticHook()
  File "/opt/airflow/plugins/hooks/elastic/elastic_hook.py", line 15, in __init__
    hosts = conn.host.splt(',')
AttributeError: 'str' object has no attribute 'splt'
[2025-05-23T13:18:28.960+0000] {taskinstance.py:1323} INFO - Marking task as FAILED. dag_id=elastic_dag, task_id=print_es_info, execution_date=20250523T131827, start_date=20250523T131828, end_date=20250523T131828
[2025-05-23T13:18:28.964+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 490 for task print_es_info ('str' object has no attribute 'splt'; 3691)
[2025-05-23T13:18:29.000+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2025-05-23T13:18:29.013+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
